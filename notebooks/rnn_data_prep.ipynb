{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, warnings, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_books(path:'str',lines_to_skip:int)->str:\n",
    "    \"\"\"Used to read in books\n",
    "\n",
    "    Args:\n",
    "        path (str): path to book location\n",
    "        lines_to_skip (int): number of lines to skip\n",
    "\n",
    "    Returns:\n",
    "        str: raw book text\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        for _ in range(lines_to_skip):\n",
    "            next(file)\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defoe books ##\n",
    "# Robinson Crusoe\n",
    "rc_text = read_in_books('../books/daniel_defoe/robinson_crusoe.txt',12)\n",
    "# Buccaneers and Marooners\n",
    "bm_text = read_in_books('../books/daniel_defoe/buccaneers_and_marooners.txt',16)\n",
    "# Captain Singleton\n",
    "cs_text = read_in_books('../books/daniel_defoe/captain_singleton.txt',4)\n",
    "\n",
    "\n",
    "## Swift books ##\n",
    "# Gulliver's Travels\n",
    "gt_text = read_in_books('../books/jonathan_swift/gullivers_travels.txt', 28)\n",
    "# Tale of a tub\n",
    "tot_text = read_in_books('../books/jonathan_swift/tale_of_a_tub.txt', 249)\n",
    "\n",
    "\n",
    "## Who is this ##\n",
    "# General History of Pyrates\n",
    "pyrates_text = read_in_books('../books/gen_history_of_the_pyrates.txt', 52)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mist_text_data_prep(text:str, cutoff:int = 50)->list:\n",
    "    \"\"\"Used to read in, clean and keep Mist Weekly Journal Sentences\n",
    "\n",
    "    Args:\n",
    "        text (str): original text\n",
    "        cutoff (int, optional): Cutoff length. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        list: List of sentences to keep\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s[:punct:]]', '', text)\n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "    \n",
    "    cleaned_sentences = []\n",
    "    valid_words = set(words.words())\n",
    "\n",
    "    for s in sentences:\n",
    "        last_item = s[-1:]\n",
    "        if last_item not in ['.','!',';','?']:\n",
    "            last_item = ''\n",
    "\n",
    "        tokens = s.split()\n",
    "        \n",
    "        filtered_tokens = [word for word in tokens if word.lower() in valid_words]\n",
    "        \n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "        filtered_text+=last_item\n",
    "\n",
    "        if len(filtered_tokens)>=cutoff:\n",
    "\n",
    "            cleaned_sentences.append(filtered_text)\n",
    "    \n",
    "    return cleaned_sentences, len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proper_nouns(text: str)->list:\n",
    "    \"\"\"Checks for proper nouns in a given text using NLTK.\n",
    "\n",
    "    Args:\n",
    "        text (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        list: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    proper_nouns = [word for word, pos in tagged if pos == 'NNP']\n",
    "\n",
    "    return proper_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_capitalization(text:str)-> str:\n",
    "    \"\"\"Some books have very poor capitalization so this looks and decides\n",
    "    if words are proper nouns and should be capitalized\n",
    "\n",
    "    Args:\n",
    "        text (str): text too look at\n",
    "\n",
    "    Returns:\n",
    "        str: Properly capitalized text\n",
    "    \"\"\"\n",
    "    \n",
    "    proper_nouns = check_proper_nouns(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text[0].upper() + text[1:]\n",
    "    \n",
    "    for noun in proper_nouns:\n",
    "        pattern = re.compile(r'\\b' + re.escape(noun.lower()) + r'\\b', re.IGNORECASE)\n",
    "        text = pattern.sub(noun, text)\n",
    "\n",
    "    text = text.replace(' i ',' I ')\n",
    "    text = text.replace(\" i'm \",\" I'm \")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_starting_with_capital(text:str)->list:\n",
    "    \"\"\"Used for EDA to find words with capital letters\n",
    "\n",
    "    Args:\n",
    "        text (str): text to look at\n",
    "\n",
    "    Returns:\n",
    "        list: list of capital words\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\b[A-Z][a-z]*\\b', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_data_prep(text:str, book:str)->list:\n",
    "    \"\"\"This function preps the text for the RNN model. Each text has its own process as they are\n",
    "    very different\n",
    "\n",
    "    Args:\n",
    "        text (str): the text of the book\n",
    "        book (str): the book\n",
    "\n",
    "    Returns:\n",
    "        list: list of setences\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    cleaned_sentences = [s.replace('\\n',' ') for s in sentences]\n",
    "    cleaned_sentences = [s.replace('_','') for s in cleaned_sentences]\n",
    "\n",
    "    if book == 'rc':\n",
    "        pass\n",
    "\n",
    "    elif book == 'gt':\n",
    "        pass\n",
    "\n",
    "    elif book == 'bm':\n",
    "        pass\n",
    "\n",
    "    elif book == 'cs':\n",
    "        pass\n",
    "\n",
    "    elif book == 'tot':\n",
    "        pass\n",
    "\n",
    "    elif book == 'pyrates':\n",
    "\n",
    "        cleaned_sentences = [fix_capitalization(s) for s in cleaned_sentences]\n",
    "\n",
    "    else:\n",
    "        warnings.warn('The book name provided - {} - has not been reviewed.'.format(book), UserWarning)\n",
    "\n",
    "\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_sentence(sentence_list:list, cutoff: int=50)->list:\n",
    "    \"\"\"Determines sentences to keep based off their length\n",
    "\n",
    "    Args:\n",
    "        sentence_list (list): list of sentences from your text\n",
    "        cutoff (int, optional): How many tokens the sentence must have to keep. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences to keep\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "    for s in sentence_list:\n",
    "        tokens = word_tokenize(s)\n",
    "        if len(tokens)>cutoff:\n",
    "            return_list.append(s)\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_sentence_list = text_data_prep(rc_text, book = 'rc')\n",
    "pyrates_sentence_list = text_data_prep(pyrates_text, book = 'pyrates')\n",
    "gt_sentence_list = text_data_prep(gt_text, book = 'gt')\n",
    "tot_sentence_list = text_data_prep(tot_text, book = 'tot')\n",
    "bm_sentence_list = text_data_prep(bm_text, book = 'bm')\n",
    "cs_sentence_list = text_data_prep(cs_text, book = 'cs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_50 = keep_sentence(rc_sentence_list)\n",
    "cs_50 = keep_sentence(cs_sentence_list)\n",
    "bm_50 = keep_sentence(bm_sentence_list)\n",
    "tot_50 = keep_sentence(tot_sentence_list)\n",
    "gt_50 = keep_sentence(gt_sentence_list)\n",
    "pyrates_50 = keep_sentence(pyrates_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178 3703\n",
      "1087 2346\n",
      "982 2874\n",
      "375 1087\n",
      "809 2603\n",
      "1126 2874\n"
     ]
    }
   ],
   "source": [
    "print(len(rc_50),len(rc_sentence_list))\n",
    "print(len(cs_50),len(cs_sentence_list))\n",
    "print(len(bm_50),len(bm_sentence_list))\n",
    "print(len(tot_50),len(tot_sentence_list))\n",
    "print(len(gt_50),len(gt_sentence_list))\n",
    "print(len(pyrates_50),len(pyrates_sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mist_text_paths = Path('../books/mist_weekly_journal_text').glob('*')\n",
    "mist_text_files = [f for f in mist_text_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mist_50 = []\n",
    "num_sentences = 0\n",
    "for f in mist_text_files:\n",
    "\n",
    "    with open(f, 'r') as file:\n",
    "        mist_text = file.read()\n",
    "\n",
    "    cleaned_50, temp_num = mist_text_data_prep(mist_text)\n",
    "\n",
    "    mist_50.extend(cleaned_50)\n",
    "    num_sentences+=temp_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615, 19363)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mist_50), num_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_data/pyrates.pkl','wb') as file:\n",
    "    pickle.dump(pyrates_50,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = rc_50 + cs_50 + bm_50  + mist_50 + tot_50 + gt_50\n",
    "labels = [0]*(len(rc_50) + len(cs_50) + len(bm_50)) + [1]*len(mist_50) +[2]*(len(tot_50) + len(gt_50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = rc_50 + bm_50  + mist_50 + tot_50 + gt_50\n",
    "labels = [0]*(len(rc_50) + len(bm_50)) + [1]*len(mist_50) +[2]*(len(tot_50) + len(gt_50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_dict = {'sentences':sentences,'labels':labels}\n",
    "with open('model_data/dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(data_set_dict,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a more balanced dataset for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3160"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rc_50 + bm_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mist_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1184"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_50+gt_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = rc_50+bm_50\n",
    "sample_df = np.random.choice(all_df, size=615, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_js = tot_50+gt_50\n",
    "sample_js = np.random.choice(all_js, size=615, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = sample_df.tolist()+mist_50+sample_js.tolist()\n",
    "sample_lables = [0]*(len(sample_df.tolist())) + [1]*len(mist_50) +[2]*(len(sample_js.tolist()))\n",
    "\n",
    "sample_data_set_dict = {'sentences':sample_sentences,'labels':sample_lables}\n",
    "with open('model_data/sample_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(sample_data_set_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = sample_df.tolist()+mist_50\n",
    "sample_lables = [0]*(len(sample_df.tolist())) + [1]*len(mist_50)\n",
    "\n",
    "sample_data_set_dict = {'sentences':sample_sentences,'labels':sample_lables}\n",
    "\n",
    "with open('model_data/sample_dataset_focused.pkl', 'wb') as file:\n",
    "    pickle.dump(sample_data_set_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_nlp",
   "language": "python",
   "name": "tensorflow_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
