{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sample data\n",
    "texts = [\"This is a great movie.\", \"I didn't like this movie.\", \"The movie was okay.\"]\n",
    "labels = [1, 2, 3]  # 1 for cat 1, 2 for cat 2 and 3 for cat 3\n",
    "\n",
    "# 1. Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# 2. Pad sequences to ensure they have the same length\n",
    "max_sequence_length = 10  # or choose based on your data\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# 3. One-hot encode the labels (3 categories)\n",
    "y = to_categorical(labels, num_classes=4)  # num_classes should be 4 to include 0 indexing (1, 2, 3)\n",
    "\n",
    "# 4. Define the RNN model with LSTM layer\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_sequence_length),\n",
    "    LSTM(64, activation='relu', return_sequences=True),  # LSTM layer\n",
    "    GlobalAveragePooling1D(),  # Pool the LSTM output\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(4, activation='softmax')  # 4 categories: 0, 1, 2, 3\n",
    "])\n",
    "\n",
    "# 5. Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Train the model\n",
    "model.fit(X, y, epochs=10)\n",
    "\n",
    "# Example of using the model to predict\n",
    "test_text = [\"I loved the film!\"]\n",
    "test_seq = tokenizer.texts_to_sequences(test_text)\n",
    "test_seq_padded = pad_sequences(test_seq, maxlen=max_sequence_length)\n",
    "\n",
    "pred = model.predict(test_seq_padded)\n",
    "predicted_class = pred.argmax(axis=-1)  # Get the index of the highest probability\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sample data\n",
    "texts = [\"This is a great movie.\", \"I didn't like this movie.\", \"The movie was okay.\"]\n",
    "labels = [1, 2, 3]  # 1 for cat 1, 2 for cat 2, and 3 for cat 3\n",
    "\n",
    "# 1. Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# 2. Pad sequences to ensure they have the same length\n",
    "max_sequence_length = 10  # You can adjust this based on the data\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# 3. One-hot encode the labels (3 categories)\n",
    "y = to_categorical(labels, num_classes=4)  # num_classes should be 4 to include 0 indexing (1, 2, 3)\n",
    "\n",
    "# 4. Load GloVe embeddings (assuming you've downloaded the file glove.6B.100d.txt)\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe vectors (e.g., 100-dimensional vectors)\n",
    "glove_file = 'glove.6B.100d.txt'  # Make sure to download and provide the correct path\n",
    "embeddings_index = load_glove_embeddings(glove_file)\n",
    "\n",
    "# 5. Prepare the embedding matrix\n",
    "embedding_dim = 100  # Choose the dimension based on the GloVe file you use (e.g., 50, 100, 200)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embeddings_index:\n",
    "        embedding_matrix[i] = embeddings_index[word]\n",
    "\n",
    "# 6. Define the model with GloVe embeddings\n",
    "model = Sequential([\n",
    "    # Use the GloVe embedding matrix (non-trainable by default)\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length,\n",
    "              weights=[embedding_matrix], trainable=False),  # Set trainable=True if you want to fine-tune\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(4, activation='softmax')  # 4 categories: 0, 1, 2, 3\n",
    "])\n",
    "\n",
    "# 7. Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 8. Train the model\n",
    "model.fit(X, y, epochs=10)\n",
    "\n",
    "# Example of using the model to predict\n",
    "test_text = [\"I loved the film!\"]\n",
    "test_seq = tokenizer.texts_to_sequences(test_text)\n",
    "test_seq_padded = pad_sequences(test_seq, maxlen=max_sequence_length)\n",
    "\n",
    "pred = model.predict(test_seq_padded)\n",
    "predicted_class = pred.argmax(axis=-1)  # Get the index of the highest probability\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
